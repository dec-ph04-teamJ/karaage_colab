{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNF+JxrVLklL1Cv16T2xqQA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 学習データ"],"metadata":{"id":"e5gFesFu2UDC"}},{"cell_type":"code","source":["import numpy as np\n","\n","practice1 = \"大変ありがたいです\"\n","practice2 = \"まじでありがとうございます！\"\n","practice3 = \"がちありがとう！\"\n","\n","text1 = '''すごい、、めちゃめちゃ助かります！\n","ありがとうございます🙇‍♂️\n","こちらを参考にしながらやってみます！'''\n","text2 = '''了解です！全然ゆっくりで大丈夫です(excited)'''\n","text3 = '''本当にありがたいです🥲🥲\n","平和取り戻しておきます笑'''\n","\n","# texts = np.array([text1, text2, text3])\n","# print(texts.shape)\n","# print(texts[0,])\n","practices = [practice1, practice2, practice3]\n","texts = [text1, text2, text3]"],"metadata":{"id":"WJMPm3eE2X-1","executionInfo":{"status":"ok","timestamp":1676652337761,"user_tz":-540,"elapsed":265,"user":{"displayName":"原龍依","userId":"06110471220578670099"}}},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["# MeCabのインストール"],"metadata":{"id":"9nzAbKI5HXMa"}},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jFsGWny4GrlU","executionInfo":{"status":"ok","timestamp":1676648011140,"user_tz":-540,"elapsed":38610,"user":{"displayName":"原龍依","userId":"06110471220578670099"}},"outputId":"722e6ead-10ed-4cba-8bec-c4d8ed8df78f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mecab-python3\n","  Downloading mecab_python3-1.0.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (577 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.8/577.8 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: mecab-python3\n","Successfully installed mecab-python3-1.0.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting unidic-lite\n","  Downloading unidic-lite-1.0.8.tar.gz (47.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: unidic-lite\n","  Building wheel for unidic-lite (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for unidic-lite: filename=unidic_lite-1.0.8-py3-none-any.whl size=47658837 sha256=f9e6036d852c72a20d52ff6fb8309a7b1efc5f8792c824e6c5a82ac9733cc7b9\n","  Stored in directory: /root/.cache/pip/wheels/8c/dd/8f/e21fc192dcd38ae31e1185ce4e66e12df4e811e3d469866e15\n","Successfully built unidic-lite\n","Installing collected packages: unidic-lite\n","Successfully installed unidic-lite-1.0.8\n"]}],"source":["!pip install mecab-python3\n","!pip install unidic-lite # 辞書インストール"]},{"cell_type":"code","source":["import MeCab\n","\n","# MeCabのオブジェクト生成\n","wakati = MeCab.Tagger(\"-Owakati\")\n","# chasen = MeCab.Tagger(\"-Ochasen\") # なんかエラー出た\n","tagger = MeCab.Tagger()\n","\n","wakati1 = wakati.parse(text1)\n","# chasen1 = chasen.parse(text1)\n","tagger1 = tagger.parse(text1)\n","print(type(wakati1))\n","print(wakati1)\n","print(type(tagger1))\n","print(tagger1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsjKKC5cHl2G","executionInfo":{"status":"ok","timestamp":1676648011141,"user_tz":-540,"elapsed":16,"user":{"displayName":"原龍依","userId":"06110471220578670099"}},"outputId":"ca753d2a-2e75-4085-bef6-485a8d75322d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'str'>\n","すごい 、 、 めちゃめちゃ 助かり ます ！ ありがとう ござい ます 🙇 ‍♂ ️ こちら を 参考 に し ながら やっ て み ます ！ \n","\n","<class 'str'>\n","すごい\tスゴイ\tスゴイ\t凄い\t形容詞-一般\t形容詞\t連体形-一般\t2\n","、\t\t\t、\t補助記号-読点\t\t\t\n","、\t\t\t、\t補助記号-読点\t\t\t\n","めちゃめちゃ\tメチャメチャ\tメチャメチャ\t目茶目茶\t副詞\t\t\t0\n","助かり\tタスカリ\tタスカル\t助かる\t動詞-一般\t五段-ラ行\t連用形-一般\t3\n","ます\tマス\tマス\tます\t助動詞\t助動詞-マス\t終止形-一般\t\n","！\t\t\t！\t補助記号-句点\t\t\t\n","ありがとう\tアリガトー\tアリガトウ\t有り難う\t感動詞-一般\t\t\t2\n","ござい\tゴザイ\tゴザル\t御座る\t動詞-非自立可能\t五段-ラ行\t連用形-イ音便\t2\n","ます\tマス\tマス\tます\t助動詞\t助動詞-マス\t終止形-一般\t\n","🙇\t🙇\t🙇\t🙇\t補助記号-一般\t\t\t0\n","‍♂\t‍♂\t‍♂\t‍♂\t記号-一般\t\t\t0\n","️\t️\t️\t️\t補助記号-一般\t\t\t0\n","こちら\tコチラ\tコチラ\t此方\t代名詞\t\t\t0\n","を\tオ\tヲ\tを\t助詞-格助詞\t\t\t\n","参考\tサンコー\tサンコウ\t参考\t名詞-普通名詞-サ変可能\t\t\t0\n","に\tニ\tニ\tに\t助詞-格助詞\t\t\t\n","し\tシ\tスル\t為る\t動詞-非自立可能\tサ行変格\t連用形-一般\t0\n","ながら\tナガラ\tナガラ\tながら\t助詞-接続助詞\t\t\t\n","やっ\tヤッ\tヤル\t遣る\t動詞-非自立可能\t五段-ラ行\t連用形-促音便\t0\n","て\tテ\tテ\tて\t助詞-接続助詞\t\t\t\n","み\tミ\tミル\t見る\t動詞-非自立可能\t上一段-マ行\t連用形-一般\t1\n","ます\tマス\tマス\tます\t助動詞\t助動詞-マス\t終止形-一般\t\n","！\t\t\t！\t補助記号-句点\t\t\t\n","EOS\n","\n"]}]},{"cell_type":"markdown","source":["# Kerasによる文章のベクトル化\n","\n"],"metadata":{"id":"r9qM2AAsy-hw"}},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","\n","# Tokenizerをインスタンス化\n","tokenizer = Tokenizer()\n","# 文章を与える．\n","tokenizer.fit_on_texts(texts)\n","\n","print(type(texts), \"length\", len(texts))\n","print(\"与えられた文章の数：\", tokenizer.document_count)\n","print(\"与えられた文章内の単語ごとの出現回数：\", tokenizer.word_counts)\n","print(\"単語ごとに割り当てられたインデックス番号\", tokenizer.word_index)\n","\n","# ベクトル化\n","  # 数字の並び順は word_index 属性で取得できた並びの順．インデックス番号 0 には単語が割り当てられていないので常に0 0 である点に注意\n","# （バイナリ表現） \n","matrix = tokenizer.texts_to_matrix(texts, \"binary\")\n","print(\"\\n\", \"binary:\", \"\\n\", matrix)\n","\n","# （カウント表現） \n","matrix = tokenizer.texts_to_matrix(texts, \"count\")\n","print(\"\\n\", \"count:\", \"\\n\", matrix)\n","\n","# （TF-IDF表現）Term Frequency - Inverse Document Frequency\n","# 文章に含まれる単語の重要度を数値化したもの \n","matrix = tokenizer.texts_to_matrix(texts, \"tfidf\")\n","print(\"\\n\", \"TF-IDF:\", \"\\n\", matrix)\n"],"metadata":{"id":"1-v7FY_kJLgO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676648014006,"user_tz":-540,"elapsed":2875,"user":{"displayName":"原龍依","userId":"06110471220578670099"}},"outputId":"a1483689-75a2-4e9b-acdd-6e9b12bf52cf"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'list'> length 3\n","与えられた文章の数： 3\n","与えられた文章内の単語ごとの出現回数： OrderedDict([('すごい、、めちゃめちゃ助かります！', 1), ('ありがとうございます🙇\\u200d♂️', 1), ('こちらを参考にしながらやってみます！', 1), ('了解です！全然ゆっくりで大丈夫です', 1), ('excited', 1), ('本当にありがたいです\\U0001f972\\U0001f972', 1), ('平和取り戻しておきます笑', 1)])\n","単語ごとに割り当てられたインデックス番号 {'すごい、、めちゃめちゃ助かります！': 1, 'ありがとうございます🙇\\u200d♂️': 2, 'こちらを参考にしながらやってみます！': 3, '了解です！全然ゆっくりで大丈夫です': 4, 'excited': 5, '本当にありがたいです\\U0001f972\\U0001f972': 6, '平和取り戻しておきます笑': 7}\n","\n"," binary: \n"," [[0. 1. 1. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 1.]]\n","\n"," count: \n"," [[0. 1. 1. 1. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 1. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 1. 1.]]\n","\n"," TF-IDF: \n"," [[0.         0.91629073 0.91629073 0.91629073 0.         0.\n","  0.         0.        ]\n"," [0.         0.         0.         0.         0.91629073 0.91629073\n","  0.         0.        ]\n"," [0.         0.         0.         0.         0.         0.\n","  0.91629073 0.91629073]]\n"]}]},{"cell_type":"markdown","source":["## 分かち書きしてtokenizerを使ってみる"],"metadata":{"id":"5Q-PcGsI4YEx"}},{"cell_type":"code","source":["# for文の使い方確認\n","for i in texts:\n","  print(i, \"\\n\")\n","\n","# N = len(texts)\n","# for i in range(N):\n","#   print(texts[i], \"\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BOhTCpNp2fB1","executionInfo":{"status":"ok","timestamp":1676648014007,"user_tz":-540,"elapsed":19,"user":{"displayName":"原龍依","userId":"06110471220578670099"}},"outputId":"976bad69-e801-479a-9d3b-375f35d50315"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["すごい、、めちゃめちゃ助かります！\n","ありがとうございます🙇‍♂️\n","こちらを参考にしながらやってみます！ \n","\n","了解です！全然ゆっくりで大丈夫です(excited) \n","\n","本当にありがたいです🥲🥲\n","平和取り戻しておきます笑 \n","\n"]}]},{"cell_type":"code","source":["N = len(texts)\n","str_texts = []\n","words_texts = []\n","for i in range(N):\n","  # 分かち書き\n","  str_texts.append(wakati.parse(texts[i]))\n","  # print(type(str_texts[i]),)\n","  print(str_texts[i])\n","  # 単語のlistにしたい\n","  words_texts.append(str_texts[i].split(' '))\n","  # print(type(words_texts[i]))\n","  # print(words_texts[i], \"\\n\")\n","# print(list_texts)\n","\n","# binary \n","tokenizer = Tokenizer() # Tokenizerの初期化\n","tokenizer.fit_on_texts(str_texts)\n","print(\"与えられた文章の数：\", tokenizer.document_count)\n","print(\"与えられた文章内の単語ごとの出現回数：\", tokenizer.word_counts)\n","print(\"単語ごとに割り当てられたインデックス番号\", tokenizer.word_index)\n","binary_texts = tokenizer.texts_to_matrix(str_texts, \"binary\")\n","print(binary_texts)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6lXaAhN_7Pxs","executionInfo":{"status":"ok","timestamp":1676648014842,"user_tz":-540,"elapsed":844,"user":{"displayName":"原龍依","userId":"06110471220578670099"}},"outputId":"55e5b580-e38e-4bf0-ad85-6c69ae3c960c"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["すごい 、 、 めちゃめちゃ 助かり ます ！ ありがとう ござい ます 🙇 ‍♂ ️ こちら を 参考 に し ながら やっ て み ます ！ \n","\n","了解 です ！ 全然 ゆっくり で 大丈夫 です ( excited ) \n","\n","本当 に ありがたい です 🥲🥲 平和 取り戻し て おき ます 笑 \n","\n","与えられた文章の数： 3\n","与えられた文章内の単語ごとの出現回数： OrderedDict([('すごい', 1), ('、', 2), ('めちゃめちゃ', 1), ('助かり', 1), ('ます', 4), ('！', 3), ('ありがとう', 1), ('ござい', 1), ('🙇', 1), ('\\u200d♂', 1), ('️', 1), ('こちら', 1), ('を', 1), ('参考', 1), ('に', 2), ('し', 1), ('ながら', 1), ('やっ', 1), ('て', 2), ('み', 1), ('了解', 1), ('です', 3), ('全然', 1), ('ゆっくり', 1), ('で', 1), ('大丈夫', 1), ('excited', 1), ('本当', 1), ('ありがたい', 1), ('\\U0001f972\\U0001f972', 1), ('平和', 1), ('取り戻し', 1), ('おき', 1), ('笑', 1)])\n","単語ごとに割り当てられたインデックス番号 {'ます': 1, '！': 2, 'です': 3, '、': 4, 'に': 5, 'て': 6, 'すごい': 7, 'めちゃめちゃ': 8, '助かり': 9, 'ありがとう': 10, 'ござい': 11, '🙇': 12, '\\u200d♂': 13, '️': 14, 'こちら': 15, 'を': 16, '参考': 17, 'し': 18, 'ながら': 19, 'やっ': 20, 'み': 21, '了解': 22, '全然': 23, 'ゆっくり': 24, 'で': 25, '大丈夫': 26, 'excited': 27, '本当': 28, 'ありがたい': 29, '\\U0001f972\\U0001f972': 30, '平和': 31, '取り戻し': 32, 'おき': 33, '笑': 34}\n","[[0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n","  1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]]\n"]}]},{"cell_type":"markdown","source":["### ver. prctices"],"metadata":{"id":"7bbArsY19eBS"}},{"cell_type":"code","source":["N = len(practices)\n","str_practices = []\n","words_practices = []\n","print(practices)\n","for i in range(N):\n","  # 分かち書き\n","  str_practices.append(wakati.parse(practices[i]))\n","  # 単語のlistにする\n","  words_practices.append(str_practices[i].split(\" \"))\n","print(str_practices)\n","\n","# 文章をbinaryベクトル化\n","tokenizer = Tokenizer() # Tokenizerの初期化\n","tokenizer.fit_on_texts(str_practices) # 文章の適用\n","print(\"与えられた文章の数：\", tokenizer.document_count)\n","print(\"与えられた文章内の単語ごとの出現回数：\", tokenizer.word_counts)\n","print(\"単語ごとに割り当てられたインデックス番号\", tokenizer.word_index)\n","binary_practices = tokenizer.texts_to_matrix(str_practices, \"binary\")\n","print(binary_practices)"],"metadata":{"id":"g4B_KBpQ7n3Z","executionInfo":{"status":"ok","timestamp":1676652341590,"user_tz":-540,"elapsed":500,"user":{"displayName":"原龍依","userId":"06110471220578670099"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d23e3e16-ea05-4327-a8b5-855f5ef978f5"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["['大変ありがたいです', 'まじでありがとうございます！', 'がちありがとう！']\n","['大変 ありがたい です \\n', 'まじ で ありがとう ござい ます ！ \\n', 'がち ありがとう ！ \\n']\n","与えられた文章の数： 3\n","与えられた文章内の単語ごとの出現回数： OrderedDict([('大変', 1), ('ありがたい', 1), ('です', 1), ('まじ', 1), ('で', 1), ('ありがとう', 2), ('ござい', 1), ('ます', 1), ('！', 2), ('がち', 1)])\n","単語ごとに割り当てられたインデックス番号 {'ありがとう': 1, '！': 2, '大変': 3, 'ありがたい': 4, 'です': 5, 'まじ': 6, 'で': 7, 'ござい': 8, 'ます': 9, 'がち': 10}\n","[[0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0.]\n"," [0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0.]\n"," [0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"]}]},{"cell_type":"markdown","source":["# ベルヌーイナイーブベイズ分類器を自作してみる\n","  - MAP推定\n","  - 事前分布はディリクレ分布（$alpha=2)\n","  - クラス数は3つ（C = 3）"],"metadata":{"id":"-brevPHkPctf"}},{"cell_type":"code","source":["professor = [\"大変ありがたいです\", \"本当にありがとうございます\"]\n","senpai = [\"まじでありがとうございます！\", \"ほんとにありがとうございます！\"]\n","friend = [\"がちありがとう！\", \"まじありがとう！\"]\n","all = [professor, senpai, friend]"],"metadata":{"id":"dyM225fIReOn","executionInfo":{"status":"ok","timestamp":1676658064768,"user_tz":-540,"elapsed":2,"user":{"displayName":"原龍依","userId":"06110471220578670099"}}},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":["## とりあえずprofessorに対してやってみる"],"metadata":{"id":"810BbIaYgGDZ"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","#やっぱ別にデータフレームにしなくてよさそう\n","# columns = [\"professor\"]\n","# professor_df = pd.DataFrame(data=professor, columns=columns)\n","# professor_df\n","# str_professor = professor_df[\"professor\"]\n","wakati_professor = []\n","for i in range(len(professor)):\n","  # 分かち書き\n","  wakati_professor.append(wakati.parse(professor[i]))\n","print(wakati_professor)\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(wakati_professor)\n","print(\"与えられた文章の数：\", tokenizer.document_count)\n","docN_professor = tokenizer.document_count\n","# print(\"与えられた文章内の単語ごとの出現回数：\", tokenizer.word_counts)\n","# wordN_professor = tokenizer.word_counts\n","print(\"単語ごとに割り当てられたインデックス：\", tokenizer.word_index)\n","wordIx_professor = tokenizer.word_index\n","binary_professor = tokenizer.texts_to_matrix(wakati_professor, \"binary\")\n","print(binary_professor)\n","len(binary_professor[0])\n","wordN_professor = np.zeros((1,len(binary_professor[0])))\n","for i in range(len(binary_professor)):\n","  wordN_professor = wordN_professor + binary_professor[i,:]\n","print(\"単語の統計値：\", wordN_professor) # それぞれの単語の統計値\n","# それぞれの単語の確率\n","alpha = 2 # 事前分布（ディリクレ分布）\n","C = 3 # クラス数\n","p_word_professor = np.zeros((1, len(binary_professor[0])))\n","p_professor = docN_professor / (docN_professor*C + C * (alpha - 1))\n","for i in range(len(binary_professor[0])):\n","  p_word_professor[0,i] = (wordN_professor[0,i] + alpha - 1) / (docN_professor + 2*(alpha-1))\n","\n","print(\"それぞれの単語の確率：\", p_word_professor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"74NsYi6rBCrT","executionInfo":{"status":"ok","timestamp":1676656136591,"user_tz":-540,"elapsed":357,"user":{"displayName":"原龍依","userId":"06110471220578670099"}},"outputId":"da8025ba-42f4-4f60-db07-14607ba5a480"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["['大変 ありがたい です \\n', '本当 に ありがとう ござい ます \\n']\n","与えられた文章の数： 2\n","単語ごとに割り当てられたインデックス： {'大変': 1, 'ありがたい': 2, 'です': 3, '本当': 4, 'に': 5, 'ありがとう': 6, 'ござい': 7, 'ます': 8}\n","[[0. 1. 1. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 1. 1. 1. 1. 1.]]\n","単語の統計値： [[0. 1. 1. 1. 1. 1. 1. 1. 1.]]\n","それぞれの単語の確率： [[0.25 0.5  0.5  0.5  0.5  0.5  0.5  0.5  0.5 ]]\n"]}]},{"cell_type":"markdown","source":["# ベルヌーイナイーブベイズ分類器っぽいのができた"],"metadata":{"id":"wxR6LaHTgiRj"}},{"cell_type":"code","source":["class NaiveBayes:\n","  # ここobject指向っぽい\n","  def __init__(self):\n","    self\n","  # def __init__(self, tokenizer):\n","  #   self.tokenizer = tokenizer\n","  \n","\n","  # categoryクラスの中の文章を分かち書き\n","  def wakatigaki_all(self, categories):\n","    wakati = MeCab.Tagger(\"-Owakati\")\n","    categories_1d = list(itertools.chain.from_iterable(categories))\n","    wakati_categories = []\n","    for i in range(len(categories_1d)):\n","      wakati_categories.append(wakati.parse(categories_1d[i]))\n","    # print(wakati_categories)\n","    return wakati_categories\n","\n","  # 各カテゴリの文章を分かち書き\n","  def wakatigaki_each(self, category):\n","    wakati = MeCab.Tagger(\"-Owakati\")\n","    wakati_category = []\n","    for i in range(len(category)):\n","      wakati_category.append(wakati.parse(category[i]))\n","    # print(wakati_category)\n","    return wakati_category\n","\n","  # 分かち書きされた文章をbinary化\n","  def binary_vector(self, categories):\n","    wakati_categories = self.wakatigaki_all(categories)\n","    wakati_category = []\n","    for i in categories:\n","      wakati_category.append(self.wakatigaki_each(i))\n","    # print(wakati_category)\n","    # print(wakati_categories)\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(wakati_categories)\n","    # print(\"すべての文章の数：\", tokenizer.document_count)\n","    N_all = tokenizer.document_count\n","    # print(\"単語ごとのインデックス：\", tokenizer.word_index)\n","    # binary_categories = tokenizer.texts_to_matrix(wakati_categories, \"binary\")\n","    binary_category = []\n","    for i in wakati_category:\n","      binary_category.append(tokenizer.texts_to_matrix(i, \"binary\"))\n","    return binary_category\n","\n","  # 各単語の統計値\n","  def statistics_word(self, categories):\n","    binary_category = self.binary_vector(categories)\n","    N_words = np.zeros((3,len(binary_category[0][0])))\n","    # print(binary_category[0][0])\n","    for i in range(len(binary_category)):\n","      for j in range(len(binary_category[0])):\n","        # print(type(binary_category[i][j]))\n","        N_words[i,:] = N_words[i,:] + binary_category[i][j]\n","    return N_words\n","\n","  # MAP推定\n","  def MAP_estimation(self, categories, alpha, C):\n","    N_words = self.statistics_word(categories)\n","    # なんかtokenaizerが変わってたのでもう一回する\n","    tokenizer = Tokenizer()\n","    tokenizer.fit_on_texts(self.wakatigaki_all(categories))\n","    N_sentences_all = tokenizer.document_count\n","    N_sentences = np.zeros((len(categories),1))\n","    for i in range(len(categories)):\n","      N_sentences[i,0] = len(categories[i])\n","    # categoryの確率\n","    p_category = np.zeros((len(categories),1))\n","    for i in range(len(categories)):\n","      p_category[i,0] = (N_sentences[i,0] + (alpha-1)) / (N_sentences_all + C * (alpha-1))\n","    # print(p_category)\n","    # 各categoryにおける各単語の確率\n","    p_words = np.zeros((len(categories), len(N_words[0])))\n","    for i in range(len(categories)):\n","      for j in range(len(N_words[0])):\n","        p_words[i,j] = (N_words[i,j] + (alpha-1)) / (N_sentences[i,0] + C * (alpha-1))\n","    return p_category, p_words\n","\n","  # テストデータを入れて確認\n","  def test(self, categories, alpha, C, test_data):\n","    p_category = self.MAP_estimation(categories, alpha, C)[0]\n","    p_words = self.MAP_estimation(categories, alpha, C)[1]\n","\n","    wakati_test = wakati.parse(test_data)\n","    tokenizer = Tokenizer() # またtokenizer作り\n","    tokenizer.fit_on_texts(self.wakatigaki_all(categories))\n","    # testdataをbinary化\n","    binary_test = tokenizer.texts_to_matrix([wakati_test], \"binary\")\n","    # どのcategoryになる確率が一番高いかを計算\n","    p_test = np.zeros((len(p_category),1))\n","    for i in range(len(p_category)):\n","      p_test[i,0] = p_category[i,0]\n","      for j in range(len(binary_test[0])):\n","        if binary_test[0][j] == 1.0:\n","          p_test[i,0] = p_test[i,0] * p_words[i][j]\n","        elif binary_test[0][j] == 0.0:\n","          p_test[i,0] = p_test[i,0] * (1 - p_words[i][j])\n","    print(\"確率最大のインデックス：\", np.argmax(p_test))\n","    return p_test\n","    "],"metadata":{"id":"KOj8-QeSSJEx","executionInfo":{"status":"ok","timestamp":1676668987834,"user_tz":-540,"elapsed":5,"user":{"displayName":"原龍依","userId":"06110471220578670099"}}},"execution_count":294,"outputs":[]},{"cell_type":"code","source":["import itertools\n","import numpy as np\n","\n","naivebayes = NaiveBayes()\n","# naivebayes.wakatigaki_each(professor)\n","# print(naivebayes.binary_vector(all))\n","# naivebayes.MAP_estimation(all, 2, 3) # これで分類器が作れた？\n","test_data = \"まじか！さすが！\"\n","naivebayes.test(all, 2, 3, test_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Tv-JclCkYyq","executionInfo":{"status":"ok","timestamp":1676668988365,"user_tz":-540,"elapsed":257,"user":{"displayName":"原龍依","userId":"06110471220578670099"}},"outputId":"13ec26ad-ab4d-440b-9295-a5fb47ccfd1c"},"execution_count":295,"outputs":[{"output_type":"stream","name":"stdout","text":["確率最大のインデックス： 2\n"]},{"output_type":"execute_result","data":{"text/plain":["array([[0.00036692],\n","       [0.00043487],\n","       [0.00309238]])"]},"metadata":{},"execution_count":295}]},{"cell_type":"code","source":[],"metadata":{"id":"cazn14tlk-Hm","executionInfo":{"status":"ok","timestamp":1676668947411,"user_tz":-540,"elapsed":2,"user":{"displayName":"原龍依","userId":"06110471220578670099"}}},"execution_count":291,"outputs":[]}]}